<h1>Automating Official Website Discovery for 1000+ Software Products: Design Logic of an Intelligent Search Script</h1>
<h2>Background: The Challenge of Moving from Manual to Automated Maintenance</h2>

<p>A software distributor was originally maintaining all products on their website manually. Facing over 1000 software products, each with different website structures, if crawlers had been written from the start, it would have been straightforward. But starting from scratch with no crawlers, achieving this overnight is not a simple task.</p>
<p>Initially, using agents like Cursor with Playwright MCP, we found that while it was possible, the efficiency was extremely poor, costs were high, and accuracy couldn't be trusted for large-scale runs. After trying several approaches, with various edge cases across 1000+ software products (disconnected official websites, products becoming sub-products, changed URLs, etc.), we realized we needed to proceed step by step.</p>
<p><strong>The first step was to recover the official websites for these software products.</strong> This is the mission of the <code>openai_web_search.py</code> script.</p>
<h2>Overall Script Architecture</h2>
<p>The core goal of this script is: <strong>automatically find the corresponding official website domain from a product name</strong>. The entire system is divided into three main layers:</p>
<ol>
<li><strong>WebSearchTool</strong>: Low-level web search tool responsible for actual searching and web page crawling</li>
<li><strong>OpenAISearchAssistant</strong>: Mid-level AI assistant responsible for optimizing search strategies and judging results</li>
<li><strong>Main Function Flow</strong>: Top-level batch processing logic responsible for reading products from the database and processing them in batches</li>
</ol>
<h2>Core Component One: WebSearchTool Class</h2>
<h3>Search Infrastructure</h3>
<p><code>WebSearchTool</code> uses <code>crawl4ai</code> as the underlying crawler engine, configured as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="bp">self</span><span class="o">.</span><span class="n">browser_config</span> <span class="o">=</span> <span class="n">BrowserConfig</span><span class="p">(</span>
    <span class="n">headless</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">run_config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">(</span>
    <span class="n">cache_mode</span><span class="o">=</span><span class="n">CacheMode</span><span class="o">.</span><span class="n">BYPASS</span><span class="p">,</span>
    <span class="n">delay_before_return_html</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>  <span class="c1"># Wait 2 seconds for page to load</span>
    <span class="n">wait_for_images</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">screenshot</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>

<p>We chose <code>crawl4ai</code> over traditional <code>requests</code> because:<br />
- Can handle JavaScript-rendered pages<br />
- Better anti-crawler resistance<br />
- Supports asynchronous operations for improved efficiency</p>
<h3>Search Execution Logic</h3>
<p>The core flow of the <code>_search_with_language</code> method:</p>
<ol>
<li>
<p><strong>Build Search URL</strong>: Use DuckDuckGo's HTML search interface<br />
<code>python
   search_url = f"https://html.duckduckgo.com/html/?q={urllib.parse.quote(query)}"</code></p>
</li>
<li>
<p><strong>Retry Mechanism</strong>: Retry up to 3 times, with increasing wait times between retries (10s, 20s, 30s)</p>
</li>
<li>Special handling for 403 errors (rate limiting)</li>
<li>
<p>Other errors are also retried but logged and continued</p>
</li>
<li>
<p><strong>Parse Search Results</strong>: Extract title, URL, and snippet from HTML</p>
</li>
<li>Handle DuckDuckGo redirect URLs (<code>/l/?uddg=...</code> format)</li>
<li>Extract real target URLs</li>
</ol>
<h3>Simple Official Domain Finding</h3>
<p>The <code>find_official_domain</code> method uses a score-based matching algorithm:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Calculate matching score</span>
<span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># Domain contains product name (case-insensitive) - highest priority</span>
<span class="k">if</span> <span class="n">product_name_clean</span> <span class="ow">in</span> <span class="n">domain_lower</span><span class="p">:</span>
    <span class="n">score</span> <span class="o">+=</span> <span class="mi">10</span>  <span class="c1"># Domain matching is the most important indicator</span>

<span class="c1"># Title contains product name</span>
<span class="k">if</span> <span class="n">product_name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="n">title</span><span class="p">:</span>
    <span class="n">score</span> <span class="o">+=</span> <span class="mi">5</span>

<span class="c1"># Title contains &quot;official&quot;</span>
<span class="k">if</span> <span class="s1">&#39;official&#39;</span> <span class="ow">in</span> <span class="n">title</span><span class="p">:</span>
    <span class="n">score</span> <span class="o">+=</span> <span class="mi">3</span>

<span class="c1"># Snippet contains product name</span>
<span class="k">if</span> <span class="n">product_name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="n">snippet</span><span class="p">:</span>
    <span class="n">score</span> <span class="o">+=</span> <span class="mi">2</span>

<span class="c1"># Common official domain patterns (.com prioritized)</span>
<span class="k">if</span> <span class="s1">&#39;.com&#39;</span> <span class="ow">in</span> <span class="n">domain</span><span class="p">:</span>
    <span class="n">score</span> <span class="o">+=</span> <span class="mi">2</span>
</code></pre></div>

<p>This method is simple and direct, but has limited accuracy because:<br />
- Cannot distinguish between official and third-party websites<br />
- Cannot handle ambiguous product names<br />
- Cannot understand product descriptions to assist judgment</p>
<h2>Core Component Two: Optimized Official Domain Finding</h2>
<h3>Why Optimization is Needed?</h3>
<p>Simple score-based matching encounters many problems in practical applications:<br />
- Product names may have multiple expressions (e.g., "Adobe Photoshop" vs "Photoshop")<br />
- Third-party websites may contain product names but are not official websites<br />
- Products may have been renamed or merged into other product lines</p>
<p>Therefore, we need to introduce <strong>LLM for intelligent judgment</strong>.</p>
<h3>Three Stages of the Optimization Process</h3>
<h4>Stage One: Extract Search Keywords</h4>
<p>The <code>extract_product_search_keywords</code> method uses LLM to convert product names into 1-3 most effective search keywords:</p>
<div class="codehilite"><pre><span></span><code><span class="n">extraction_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;Please extract 1-3 English search keywords for the following product name. These keywords should be the ones most likely to find the official website.</span>

<span class="s2">Requirements:</span>
<span class="s2">1. If it&#39;s a Chinese product name, first translate to the official English name</span>
<span class="s2">2. Extract 1-3 keyword combinations, prioritized from high to low:</span>
<span class="s2">   - Company name + Product name (e.g., &quot;adobe photoshop&quot;)</span>
<span class="s2">   - Company name (e.g., &quot;adobe&quot;)</span>
<span class="s2">   - Product name (e.g., &quot;photoshop&quot;)</span>
<span class="s2">3. Only use official English names, avoid common words</span>
</code></pre></div>

<p>The key to this stage is: <strong>finding keyword combinations that are most likely to find the official website</strong>, not the most complete product name.</p>
<h4>Stage Two: Sequential Search and Batch Judgment</h4>
<p>The core logic of the <code>find_official_domain_optimized</code> method:</p>
<ol>
<li>
<p><strong>Sequential Keyword Search</strong>: Search each keyword in priority order<br />
<code>python
   for keyword_idx, keyword in enumerate(search_keywords, 1):
       results = await self.search(keyword, num_results=num_results_per_keyword)</code></p>
</li>
<li>
<p><strong>Extract Candidate Domains</strong>: Extract all possible domains from search results</p>
</li>
<li>Handle DuckDuckGo redirect URLs</li>
<li>
<p>Deduplicate across keywords (keep each domain only once)</p>
</li>
<li>
<p><strong>Batch LLM Judgment</strong>: Submit all candidate domains to LLM for judgment at once<br />
   ```python<br />
   async def judge_candidates(candidates: List[Dict[str, str]]) -&gt; List[Dict[str, Any]]:<br />
       judgment_prompt = f"""Please judge whether the following websites are the official website of product "{product_name}".</p>
<p>Please carefully analyze each website to determine if it's the official website of this product. Consider:<br />
   1. Whether the domain is related to the product name<br />
   2. Whether the title and snippet match the product name and description<br />
   3. Whether it looks like an official website (not third-party, news, dictionary sites, etc.)</p>
<p>Please return a JSON array, each element corresponding to a website's judgment result:<br />
   [<br />
       {{<br />
           "domain": "domain1",<br />
           "is_official": true/false,<br />
           "confidence": 0.0-1.0,<br />
           "reason": "judgment reason"<br />
       }},<br />
       ...<br />
   ]<br />
   """<br />
   ```</p>
</li>
<li>
<p><strong>Early Termination Strategy</strong>: If an official domain with confidence &gt; 0.9 is found, return immediately, skipping remaining keywords<br />
<code>python
   if best_domain and best_confidence &gt; 0.9:
       print(f"✅ Found high-confidence official domain: {best_domain}")
       return best_domain</code></p>
</li>
</ol>
<h3>Advantages of Batch Judgment</h3>
<p>Why batch judgment instead of one-by-one?</p>
<ol>
<li><strong>Cost Efficiency</strong>: One API call can judge multiple candidate domains, saving costs compared to individual calls</li>
<li><strong>Context Consistency</strong>: LLM can compare multiple candidate domains simultaneously for more accurate judgment</li>
<li><strong>Speed Improvement</strong>: Reducing API calls improves overall processing speed</li>
</ol>
<h3>Error Handling and Rate Limiting</h3>
<p>The script implements comprehensive error handling:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># If encountering 403 error, wait longer (30-60 seconds)</span>
<span class="k">if</span> <span class="s2">&quot;403&quot;</span> <span class="ow">in</span> <span class="n">error_msg</span> <span class="ow">or</span> <span class="s2">&quot;Forbidden&quot;</span> <span class="ow">in</span> <span class="n">error_msg</span><span class="p">:</span>
    <span class="n">delay</span> <span class="o">=</span> <span class="mi">30</span> <span class="o">+</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>  <span class="c1"># 30-60 second random delay</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">delay</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">keyword</span> <span class="o">!=</span> <span class="n">search_keywords</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
    <span class="c1"># Other errors also add delay (15-20 seconds)</span>
    <span class="n">delay</span> <span class="o">=</span> <span class="mi">15</span> <span class="o">+</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">delay</span><span class="p">)</span>
</code></pre></div>

<p>Key design points:<br />
- <strong>Random Delays</strong>: Avoid being identified as bot behavior<br />
- <strong>Tiered Delays</strong>: 403 errors wait longer<br />
- <strong>Continue Execution</strong>: Even if one keyword fails, continue processing the next</p>
<h2>Core Component Three: Main Function Batch Processing Flow</h2>
<h3>Product Filtering Strategy</h3>
<p>The <code>main_async</code> function implements intelligent product filtering logic:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Default behavior: Skip products with existing URLs, and products processed within 1 day</span>
<span class="c1"># But prioritize: products without URLs but with timestamps (representing failed fetches)</span>
<span class="n">failed_products</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Products without URLs but with timestamps (prioritize)</span>
<span class="n">new_products</span> <span class="o">=</span> <span class="p">[]</span>     <span class="c1"># Products with no records at all</span>

<span class="k">for</span> <span class="n">product_id</span> <span class="ow">in</span> <span class="n">all_product_ids</span><span class="p">[:</span><span class="n">args</span><span class="o">.</span><span class="n">limit</span> <span class="o">*</span> <span class="mi">3</span><span class="p">]:</span>
    <span class="n">url_info</span> <span class="o">=</span> <span class="n">get_product_url_with_timestamp</span><span class="p">(</span><span class="n">conn</span><span class="p">,</span> <span class="n">product_id</span><span class="p">)</span>

    <span class="c1"># If URL exists, skip</span>
    <span class="k">if</span> <span class="n">url_info</span> <span class="ow">and</span> <span class="n">url_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;url&#39;</span><span class="p">):</span>
        <span class="n">skip_count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">continue</span>

    <span class="c1"># If processed within 1 day, skip (unless failed)</span>
    <span class="k">if</span> <span class="n">url_info</span> <span class="ow">and</span> <span class="n">url_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;last_fetched_at&#39;</span><span class="p">):</span>
        <span class="n">last_fetched</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">fromisoformat</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">last_fetched</span><span class="o">.</span><span class="n">isoformat</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">one_day_ago</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">url_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;url&#39;</span><span class="p">):</span>
                <span class="n">failed_products</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">product_id</span><span class="p">)</span>  <span class="c1"># Prioritize failed ones</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">skip_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">continue</span>

    <span class="c1"># Categorize</span>
    <span class="k">if</span> <span class="n">url_info</span> <span class="ow">and</span> <span class="n">url_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;last_fetched_at&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">url_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;url&#39;</span><span class="p">):</span>
        <span class="n">failed_products</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">product_id</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">url_info</span><span class="p">:</span>
        <span class="n">new_products</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">product_id</span><span class="p">)</span>

<span class="c1"># Prioritize failed products, then process new products</span>
<span class="n">product_ids_to_process</span> <span class="o">=</span> <span class="p">(</span><span class="n">failed_products</span> <span class="o">+</span> <span class="n">new_products</span><span class="p">)[:</span><span class="n">args</span><span class="o">.</span><span class="n">limit</span><span class="p">]</span>
</code></pre></div>

<p>Advantages of this strategy:<br />
1. <strong>Avoid Duplicate Processing</strong>: Products with existing URLs are skipped directly<br />
2. <strong>Prioritize Retrying Failures</strong>: Previously failed products are prioritized<br />
3. <strong>Time Window Control</strong>: Products processed within 1 day are skipped (unless failed)</p>
<h3>Processing Flow</h3>
<p>The processing flow for each product:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. Get product information</span>
<span class="n">product_data</span> <span class="o">=</span> <span class="n">get_latest_version</span><span class="p">(</span><span class="n">conn</span><span class="p">,</span> <span class="n">product_id</span><span class="p">)</span>
<span class="n">product_name</span> <span class="o">=</span> <span class="n">product_data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="n">product_desc1</span> <span class="o">=</span> <span class="n">product_data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;desc1&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="s1">&#39;&#39;</span>

<span class="c1"># 2. Find official domain</span>
<span class="n">official_domain</span> <span class="o">=</span> <span class="k">await</span> <span class="n">find_official_domain_optimized</span><span class="p">(</span>
    <span class="n">product_name</span><span class="o">=</span><span class="n">product_name</span><span class="p">,</span>
    <span class="n">product_desc1</span><span class="o">=</span><span class="n">product_desc1</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">api_key</span>
<span class="p">)</span>

<span class="c1"># 3. Save results</span>
<span class="k">if</span> <span class="n">official_domain</span><span class="p">:</span>
    <span class="n">official_url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;https://</span><span class="si">{</span><span class="n">official_domain</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">set_product_url</span><span class="p">(</span><span class="n">conn</span><span class="p">,</span> <span class="n">product_id</span><span class="p">,</span> <span class="n">official_url</span><span class="p">)</span>
    <span class="n">success_count</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># Official domain not found, clear existing URL but update fetch time</span>
    <span class="n">delete_product_url</span><span class="p">(</span><span class="n">conn</span><span class="p">,</span> <span class="n">product_id</span><span class="p">)</span>
    <span class="n">update_product_url_fetched_time</span><span class="p">(</span><span class="n">conn</span><span class="p">,</span> <span class="n">product_id</span><span class="p">)</span>
    <span class="n">cleared_count</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div>

<p>Key design points:<br />
- <strong>Update timestamp even on failure</strong>: Avoid infinite retries of the same product<br />
- <strong>Clear invalid URLs</strong>: If official domain not found, clear existing incorrect URLs<br />
- <strong>Statistics</strong>: Record counts of success, failure, and skipped</p>
<h2>Technical Details: URL Processing and Domain Extraction</h2>
<h3>DuckDuckGo Redirect Handling</h3>
<p>DuckDuckGo uses redirect URLs to protect user privacy, in the format <code>/l/?uddg=...</code>. The script needs to extract the real target URL:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Handle DuckDuckGo redirect URLs</span>
<span class="k">if</span> <span class="n">url</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;//duckduckgo.com/l/&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">url</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;/l/?&#39;</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">parsed</span> <span class="o">=</span> <span class="n">urllib</span><span class="o">.</span><span class="n">parse</span><span class="o">.</span><span class="n">urlparse</span><span class="p">(</span><span class="n">url</span> <span class="k">if</span> <span class="n">url</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;//&#39;</span><span class="p">)</span> <span class="k">else</span> <span class="sa">f</span><span class="s2">&quot;https:</span><span class="si">{</span><span class="n">url</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">urllib</span><span class="o">.</span><span class="n">parse</span><span class="o">.</span><span class="n">parse_qs</span><span class="p">(</span><span class="n">parsed</span><span class="o">.</span><span class="n">query</span><span class="p">)</span>
        <span class="k">if</span> <span class="s1">&#39;uddg&#39;</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
            <span class="n">real_url</span> <span class="o">=</span> <span class="n">urllib</span><span class="o">.</span><span class="n">parse</span><span class="o">.</span><span class="n">unquote</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;uddg&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;⚠️  URL parsing failed: </span><span class="si">{</span><span class="n">url</span><span class="si">}</span><span class="s2">, error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">continue</span>
</code></pre></div>

<h3>Domain Extraction Logic</h3>
<p>The <code>_extract_domain_from_url</code> method handles various URL formats:</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">_extract_domain_from_url</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="c1"># 1. Handle DuckDuckGo redirects</span>
    <span class="k">if</span> <span class="n">url</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;//duckduckgo.com/l/&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">url</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;/l/?&#39;</span><span class="p">):</span>
        <span class="c1"># Extract real URL from uddg parameter</span>
        <span class="o">...</span>

    <span class="c1"># 2. Skip DuckDuckGo domains</span>
    <span class="k">if</span> <span class="s1">&#39;duckduckgo.com&#39;</span> <span class="ow">in</span> <span class="n">url</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="c1"># 3. If URL has no protocol, add https://</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">url</span><span class="o">.</span><span class="n">startswith</span><span class="p">((</span><span class="s1">&#39;http://&#39;</span><span class="p">,</span> <span class="s1">&#39;https://&#39;</span><span class="p">)):</span>
        <span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;https://</span><span class="si">{</span><span class="n">url</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="c1"># 4. Parse domain</span>
    <span class="n">parsed</span> <span class="o">=</span> <span class="n">urlparse</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="n">domain</span> <span class="o">=</span> <span class="n">parsed</span><span class="o">.</span><span class="n">netloc</span>

    <span class="c1"># 5. Remove www. prefix and port number</span>
    <span class="k">if</span> <span class="n">domain</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;www.&#39;</span><span class="p">):</span>
        <span class="n">domain</span> <span class="o">=</span> <span class="n">domain</span><span class="p">[</span><span class="mi">4</span><span class="p">:]</span>
    <span class="k">if</span> <span class="s1">&#39;:&#39;</span> <span class="ow">in</span> <span class="n">domain</span><span class="p">:</span>
        <span class="n">domain</span> <span class="o">=</span> <span class="n">domain</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># 6. Validate domain format</span>
    <span class="k">if</span> <span class="s1">&#39;.&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">domain</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">domain</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="n">domain</span>
</code></pre></div>

<h2>Practical Application Results</h2>
<p>How does this script perform in practical applications?</p>
<h3>Advantages</h3>
<ol>
<li><strong>High Automation</strong>: Can batch process large numbers of products without manual intervention</li>
<li><strong>Improved Accuracy</strong>: Using LLM judgment is more accurate than simple keyword matching</li>
<li><strong>Controllable Costs</strong>: Batch judgment and early termination strategies reduce API call costs</li>
<li><strong>Error Recovery</strong>: Comprehensive retry mechanisms and error handling</li>
</ol>
<h3>Challenges</h3>
<ol>
<li><strong>Rate Limiting</strong>: DuckDuckGo and target websites may limit crawler access</li>
<li><strong>Edge Cases</strong>: Product renaming, mergers, disconnected official websites require manual handling</li>
<li><strong>Costs</strong>: Although optimized, LLM API calls still have costs</li>
</ol>
<h3>Improvement Directions</h3>
<ol>
<li><strong>Caching Mechanism</strong>: Cache processed products to avoid duplicate processing</li>
<li><strong>Concurrency Control</strong>: Improve concurrent processing while respecting rate limits</li>
<li><strong>Result Validation</strong>: Periodically verify if saved URLs are still valid</li>
</ol>
<h2>Conclusion</h2>
<p>The <code>openai_web_search.py</code> script demonstrates a practical automation solution that:</p>
<ol>
<li><strong>Layered Design</strong>: Separates search, judgment, and batch processing, each with clear responsibilities</li>
<li><strong>Intelligent Judgment</strong>: Uses LLM for semantic understanding, not simple keyword matching</li>
<li><strong>Cost Optimization</strong>: Batch processing and early termination strategies reduce API costs</li>
<li><strong>Error Handling</strong>: Comprehensive retry mechanisms and error recovery strategies</li>
<li><strong>Practical Usability</strong>: Designed for real-world scenarios with 1000+ software products, handling various edge cases</li>
</ol>
<p>This script not only solves the specific problem of "recovering official websites" but more importantly demonstrates how to balance automation level, accuracy, and cost in complex real-world scenarios.</p>
<hr />
<p><em>This article details the design logic of a script for automating the recovery of software official websites, hoping to help developers building similar automation systems.</em></p>