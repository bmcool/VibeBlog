<h1>自我演化的 Agent：讓 AI 自動長出「手腳」的架構設計</h1>
<h2>核心概念：從固定基底到動態演化</h2>

<p>傳統的 AI Agent 系統通常需要預先定義好所有工具和能力，但這在面對複雜多變的任務時會遇到瓶頸。<strong>自我演化的 Agent</strong> 採用不同的思路：</p>
<ul>
<li><strong>LLM + 固定基底（executor / storage）永遠不變</strong>：核心大腦和執行環境保持穩定</li>
<li><strong>透過不斷 loop 任務 → 自動長出新的「手腳」（tools / 小 agent）</strong>：遇到新需求時自動產生新能力</li>
<li><strong>這些手腳會被存起來，下次遇到類似任務就直接拿來用</strong>：累積知識庫，越用越強</li>
<li><strong>「訓練資料」是：任務描述 + 你 100% 確定的正確結果（大概 20 組起跳）</strong>：不需要大量資料，只需要高品質的標準答案</li>
</ul>
<p>這個構想本質上是在做：</p>
<blockquote>
<p>用「任務失敗 → 分析失敗原因 → 產生新能力 → 再嘗試」的程序搜尋（program search），而不是改 LLM 權重。</p>
</blockquote>
<h2>架構設計：六個核心組件</h2>
<p>整個系統由六個核心組件組成，形成一個完整的自我演化循環：</p>
<h3>1. Planner Agent（規劃者）</h3>
<p><strong>職責</strong>：分析任務，產出解題計畫</p>
<ul>
<li><strong>輸入</strong>：任務描述 + 目前已註冊的工具列表</li>
<li><strong>輸出</strong>：<ul>
<li>一個「解題計畫」：需要哪些步驟</li>
<li>用到哪些現有工具</li>
<li>哪些能力看起來「缺失」</li>
</ul>
</li>
</ul>
<p>例如對「找出 Adobe 所有產品」的任務，它可能產出：</p>
<pre><code class="language-text">計畫：
1. 我需要「搜尋網路」的能力 → 看工具庫有沒有
2. 如果沒有 search_web，就先建立一個
3. 然後用 search_web(&#39;Adobe all products&#39;) 找到官方產品頁
4. 下載該頁內容後，解析產品清單
</code></pre>
<h3>2. Capability Checker（能力檢查器）</h3>
<p><strong>職責</strong>：根據 Planner 的計畫，檢查所需能力是否存在</p>
<ul>
<li>檢查需要的工具是否已存在（Tool Registry）</li>
<li>如果某個能力不存在（例如 <code>search_web</code>），就標記為需要建造</li>
</ul>
<h3>3. Tool Builder Agent（造工具的小工）</h3>
<p><strong>職責</strong>：根據「需求描述」+ 你允許的依賴，產生 Python 工具檔</p>
<p>例如需求：</p>
<pre><code class="language-text">需求：建立一個 search_web(query: str) -&gt; str 的函數，
用 requests + BeautifulSoup 把 Google 搜尋結果前幾個連結的 HTML 抓回來。
先做 MVP 就好，錯了再改。
</code></pre>
<p>它輸出的是 Python 原始碼：</p>
<pre><code class="language-python"># tools/search_web.py
import requests
from bs4 import BeautifulSoup

def search_web(query: str) -&gt; str:
    # 簡單用某個搜尋 API 或直接打特定網站...
    ...
</code></pre>
<p>然後由 Executor 在 sandbox 裡跑簡單測試（例如 basic import，簡單呼叫測試）。</p>
<h3>4. Executor（執行器）</h3>
<p><strong>職責</strong>：有了工具後，照 Planner 所列的步驟跑整個 pipeline</p>
<ol>
<li>用 <code>search_web</code> 找到 Adobe 產品頁 URL</li>
<li>抓取網頁 HTML</li>
<li>用內建/現有 parser 清洗出產品清單</li>
</ol>
<p>產生<strong>最終輸出 result</strong>。</p>
<h3>5. Evaluator（評估器）</h3>
<p><strong>職責</strong>：拿 <code>result</code> 跟你提供的 <code>gold_output</code> 做比對</p>
<ul>
<li>完全符合 → 任務成功，紀錄「這套工具組合 OK」</li>
<li>不符合 → 任務失敗 → 丟給 Diagnoser 分析問題</li>
</ul>
<p>支援多種評估方法：</p>
<ul>
<li><code>exact_match</code>：完全匹配</li>
<li><code>partial_match</code>：部分匹配（計算關鍵字匹配度）</li>
<li><code>semantic_similarity</code>：使用 LLM 評估語義相似度</li>
</ul>
<h3>6. Diagnoser（診斷器）</h3>
<p><strong>職責</strong>：看 log、錯誤訊息、差異，總結失敗原因並推薦下一步</p>
<p>例如：</p>
<ul>
<li>「search_web 只能抓靜態頁面，但該頁是動態網站 → 需要 render_dynamic_page 能力」</li>
<li>「HTML 很亂，裡面有很多無關資訊 → 需要一個更強的 clean_html / extract_products 工具」</li>
</ul>
<p>然後回到 <strong>Tool Builder</strong> → 產生新工具 → 再跑一次任務。</p>
<p>這就是構想中的演化循環：</p>
<ul>
<li><strong>STEP1</strong>：缺搜尋 → 生 <code>search_web</code></li>
<li><strong>STEP2</strong>：缺動態渲染 → 生 <code>render_dynamic_page</code></li>
<li><strong>STEP3</strong>：缺清洗 → 生 <code>clean_product_list</code></li>
</ul>
<h2>固定不變的基底</h2>
<p>這些是「基底」，先建好，以後不動它：</p>
<h3>1. LLM API 本身</h3>
<p>就是現在在用的 GPT 類，作為「大腦」。</p>
<h3>2. Python 執行環境（sandbox）</h3>
<ul>
<li>可以動態建立檔案、import 新的工具</li>
<li>可以限制權限（避免亂刪檔、亂打 API）</li>
</ul>
<h3>3. 工具註冊中心（Tool Registry）</h3>
<p>一個簡單的結構，例如：</p>
<pre><code class="language-json">{
  &quot;search_web&quot;: {
    &quot;type&quot;: &quot;python_function&quot;,
    &quot;path&quot;: &quot;tools/search_web.py&quot;,
    &quot;signature&quot;: &quot;search_web(query: str) -&gt; str&quot;,
    &quot;description&quot;: &quot;用 HTTP 抓靜態網頁&quot;
  },
  &quot;render_dynamic_page&quot;: {
    &quot;type&quot;: &quot;python_function&quot;,
    &quot;path&quot;: &quot;tools/render_dynamic_page.py&quot;,
    &quot;signature&quot;: &quot;render_dynamic_page(url: str) -&gt; str&quot;,
    &quot;description&quot;: &quot;用 Playwright/Selenium 把動態網站渲染成 HTML&quot;
  }
}
</code></pre>
<p>由 agent 來「新增」與「修改」這個列表裡的工具。</p>
<h3>4. 任務與標準答案 Dataset</h3>
<p>你準備的 20 組（或更多）：</p>
<pre><code class="language-json">{
  &quot;task_id&quot;: &quot;adobe_products&quot;,
  &quot;instruction&quot;: &quot;找出 Adobe 所有的產品，列表輸出&quot;,
  &quot;gold_output&quot;: &quot;...你確認過正確的結果...&quot;,
  &quot;eval_method&quot;: &quot;exact_match / 部分比對策略&quot;
}
</code></pre>
<h2>訓練方法：從 20 組任務到穩定系統</h2>
<p>這裡的「訓練」可以分兩層：</p>
<h3>層一：實際工具 / Agent 流程的「搜尋 + 篩選」</h3>
<p>這是你一定做得到的：</p>
<ul>
<li><p>每個任務：</p>
<ul>
<li>一直跑「嘗試 → 失敗 → 生新工具 → 再試 → 評估」的 loop</li>
<li><strong>只保留那些能通過評估的版本</strong>（工具代碼 + 使用流程）</li>
</ul>
</li>
<li><p>久了之後，你會得到：</p>
<ul>
<li>一組 GPT prompt + 一組工具庫 + 一套調用策略，可以穩定解決這 20 組任務</li>
</ul>
</li>
</ul>
<p>這有點像是：</p>
<blockquote>
<p>在搜尋「程式 + 工具組合」，而 Dataset 是你的 20 題考題。</p>
</blockquote>
<h3>層二：Meta-學習（可選）</h3>
<p>如果你有權限做「微調」或「提示工程」，可以進一步做：</p>
<ol>
<li><p><strong>把成功的 log 存成「示範資料」</strong></p>
<ul>
<li>Prompt：任務描述 + 工具列表 + log</li>
<li>Output：最終成功的工具調用計畫 / chain-of-thought</li>
</ul>
</li>
<li><p>拿這些示範：</p>
<ul>
<li>當成「系統 prompt 裡的 few-shot example」</li>
<li>或者丟去做微調（如果你能訓練自己的 model）</li>
</ul>
</li>
</ol>
<p>這樣下次遇到「類似任務」，Planner 一開始就比較有 sense，不用瞎猜那麼多輪。</p>
<h2>讓 loop 真的「長出小 agent」</h2>
<p>你想要的是：新生出來的不只是 function，而是「小 agent」。實作上可以：</p>
<h3>把「能力」抽象為 Agent，而不是一個函式</h3>
<p>例如：</p>
<ul>
<li><p><code>SearchAgent</code></p>
<ul>
<li>內部會呼叫 <code>search_web</code>、<code>render_dynamic_page</code> 等工具</li>
<li>對外只有一個介面：<code>search_and_extract(query) -&gt; structured_data</code></li>
</ul>
</li>
<li><p><code>ProductExtractorAgent</code></p>
<ul>
<li>專門負責從 HTML 中抽出產品名稱、分類等。</li>
</ul>
</li>
</ul>
<p><strong>Loop 的演化方向：</strong></p>
<ol>
<li><p>一開始只是「functions」：</p>
<ul>
<li><code>search_web</code></li>
<li><code>render_dynamic_page</code></li>
<li><code>parse_products</code></li>
</ul>
</li>
<li><p>當這些 function 穩定後，Planner 可以發現：</p>
<ul>
<li>「每次做『找產品』任務時，都要呼叫這三個工具，順序差不多」</li>
<li>然後由 LLM 自己提出：<ul>
<li>「把它封裝成一個 <code>ProductSearchAgent</code> 吧」</li>
</ul>
</li>
</ul>
</li>
<li><p>Tool Builder 產出一個 class / module：</p>
</li>
</ol>
<pre><code class="language-python"># agents/product_search_agent.py
from tools.search_web import search_web
from tools.render_dynamic_page import render_dynamic_page
from tools.parse_products import parse_products

class ProductSearchAgent:
    def run(self, brand: str) -&gt; list[str]:
        # 1. search_web 找到 brand 產品頁
        # 2. render_dynamic_page 渲染
        # 3. parse_products 萃取
        ...
</code></pre>
<ol start="4">
<li>Tool Registry 這時也把 <code>ProductSearchAgent</code> 視作一個「能力」，Planner 下次可以直接叫它，不用重新設計 pipeline。</li>
</ol>
<p>這樣就符合你說的：</p>
<blockquote>
<p>手腳甚至包含其他小 agent。</p>
</blockquote>
<p>本質上就是：</p>
<p><strong>從「常用工具組合」→ 自動封裝成一個新 agent，並註冊進能力圖譜裡。</strong></p>
<h2>現實要注意的三個坑</h2>
<h3>1. 錯誤的自信 / 幻覺</h3>
<p>LLM 很容易「覺得自己做對了」，但實際結果不對。</p>
<p>所以<strong>一定要以「可計算的評估」為主</strong>（你提供的 gold output），不要相信 LLM 自己說的「我覺得這次結果 OK」。</p>
<h3>2. 工具品質很爛歪樓</h3>
<p>如果沒測試，它會寫出能跑但邏輯很歪的 code。</p>
<p>至少要有：</p>
<ul>
<li>基本單元測試（型別、簡單案例）</li>
<li>效果測試（試跑 2–3 個 sample 任務）</li>
</ul>
<h3>3. 工具爆炸 / 能力重複</h3>
<p>時間久了會長出一堆類似功能的工具。</p>
<p>你可以週期性跑一個「Refactor Agent」：</p>
<ul>
<li>看哪些工具功能重疊</li>
<li>提議合併 / 清理</li>
<li>類似程式碼 refactoring。</li>
</ul>
<h2>實作建議：從最小版本開始</h2>
<p>如果你要開始實作這套 loop，我會建議第一步：</p>
<ol>
<li><p><strong>準備那 20 個「任務 + 標準答案」</strong>（JSON）</p>
</li>
<li><p>建一個最小版的：</p>
<ul>
<li>Tool Registry（可以先手動寫幾個工具）</li>
<li>Executor</li>
<li>Evaluator（比對 result vs gold）</li>
</ul>
</li>
<li><p>先讓 LLM 當：</p>
<ul>
<li>Planner + Diagnoser，但先<strong>不要讓它自改 code</strong></li>
<li>先觀察它會怎麼提出「我缺什麼能力」，你再人工寫工具</li>
</ul>
</li>
<li><p>等這套流程穩定後，再把「人工寫工具」一步換成 Tool Builder + Python 寫檔。</p>
</li>
</ol>
<h2>實際應用場景</h2>
<p>這個架構特別適合：</p>
<ol>
<li><strong>需要不斷適應新資料來源的任務</strong>：例如從不同網站抓取產品資訊</li>
<li><strong>任務類型相似但細節不同的場景</strong>：例如「找出 X 公司的所有產品」</li>
<li><strong>需要組合多種能力的複雜任務</strong>：例如「搜尋 → 解析 → 清洗 → 格式化」</li>
</ol>
<h2>總結</h2>
<p>自我演化的 Agent 架構提供了一個新的思路：<strong>不是預先定義所有能力，而是讓系統在執行任務的過程中自動長出所需的能力</strong>。</p>
<p>這種方法的優勢：</p>
<ul>
<li><strong>適應性強</strong>：遇到新需求時自動演化</li>
<li><strong>可累積</strong>：每次解決任務都會留下可重用的工具</li>
<li><strong>成本可控</strong>：只需要 20 組高品質的訓練資料，不需要大量標註</li>
</ul>
<p>當然，這也帶來新的挑戰：</p>
<ul>
<li>需要更嚴格的評估機制</li>
<li>需要防止工具品質問題</li>
<li>需要管理工具庫的複雜度</li>
</ul>
<p>但整體來說，這是一個值得探索的方向，特別是在需要處理多樣化任務的場景中。</p>
